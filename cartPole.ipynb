{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=4, hidden_dim=128, action_dim=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 輸出 logits (對應 action_dim=2)\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        給定單一狀態 (shape: (4,))，\n",
    "        輸出一個 action 與 log_prob。\n",
    "        \"\"\"\n",
    "        # 轉成 batch=1 的張量\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # shape (1, 4)\n",
    "        logits = self.forward(state)  # shape (1, 2)\n",
    "\n",
    "        # 用 Categorical 分佈做采樣\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()          # 得到一個整數 0 or 1\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    給定一串 step 的 reward，例如 [r0, r1, r2, ...]，\n",
    "    回傳每個 time step t 對應的折扣後回報 G_t。\n",
    "    \"\"\"\n",
    "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add = rewards[t] + gamma * running_add\n",
    "        discounted[t] = running_add\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy_net, gamma=0.99):\n",
    "    \"\"\"\n",
    "    跑一個 episode，收集 (log_prob, reward)。\n",
    "    回傳：\n",
    "      - log_probs: list of log_prob (tensor)\n",
    "      - rewards: list of float\n",
    "      - total_reward: episode 最後得到的累積 reward (評估用)\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()[0]  # 若是 gymnasium，env.reset() 回傳 (obs, info)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, log_prob = policy_net.get_action(state)\n",
    "        \n",
    "        # 執行動作\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # 紀錄\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # 計算折扣後回報\n",
    "    discounted_r = discount_rewards(rewards, gamma)  # shape (episode_length,)\n",
    "    \n",
    "    return log_probs, discounted_r, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cartpole(\n",
    "    max_episodes=1000, \n",
    "    gamma=0.99, \n",
    "    lr=1e-3, \n",
    "    hidden_dim=128\n",
    "):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # 環境狀態維度=4，動作維度=2\n",
    "    policy_net = PolicyNetwork(state_dim=4, hidden_dim=hidden_dim, action_dim=2)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        log_probs, discounted_r, total_reward = run_episode(env, policy_net, gamma)\n",
    "        \n",
    "        # 準備計算 policy gradient loss\n",
    "        # Σ_t [ -log_pi(a_t|s_t) * G_t ]\n",
    "        # G_t 就是 discounted_r[t]\n",
    "        loss = 0\n",
    "        for log_prob, Gt in zip(log_probs, discounted_r):\n",
    "            loss += -log_prob * Gt\n",
    "\n",
    "        # 反向傳播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 顯示訓練進度\n",
    "        print(f\"Episode {episode}, Reward = {total_reward}\")\n",
    "        \n",
    "        # 如果總分連續多次都達到滿分(500)，可以提早結束\n",
    "        if total_reward >= 500: \n",
    "            print(\"Solved CartPole!\")\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return policy_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy_net, episodes=5):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = policy_net.get_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    env.close()\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Test over {episodes} episodes, average reward: {avg_reward:.2f}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rlenv/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward = 16.0\n",
      "Episode 1, Reward = 14.0\n",
      "Episode 2, Reward = 19.0\n",
      "Episode 3, Reward = 15.0\n",
      "Episode 4, Reward = 16.0\n",
      "Episode 5, Reward = 25.0\n",
      "Episode 6, Reward = 33.0\n",
      "Episode 7, Reward = 16.0\n",
      "Episode 8, Reward = 27.0\n",
      "Episode 9, Reward = 17.0\n",
      "Episode 10, Reward = 11.0\n",
      "Episode 11, Reward = 11.0\n",
      "Episode 12, Reward = 19.0\n",
      "Episode 13, Reward = 22.0\n",
      "Episode 14, Reward = 45.0\n",
      "Episode 15, Reward = 12.0\n",
      "Episode 16, Reward = 15.0\n",
      "Episode 17, Reward = 31.0\n",
      "Episode 18, Reward = 13.0\n",
      "Episode 19, Reward = 22.0\n",
      "Episode 20, Reward = 15.0\n",
      "Episode 21, Reward = 15.0\n",
      "Episode 22, Reward = 26.0\n",
      "Episode 23, Reward = 44.0\n",
      "Episode 24, Reward = 31.0\n",
      "Episode 25, Reward = 22.0\n",
      "Episode 26, Reward = 11.0\n",
      "Episode 27, Reward = 26.0\n",
      "Episode 28, Reward = 58.0\n",
      "Episode 29, Reward = 28.0\n",
      "Episode 30, Reward = 21.0\n",
      "Episode 31, Reward = 18.0\n",
      "Episode 32, Reward = 26.0\n",
      "Episode 33, Reward = 28.0\n",
      "Episode 34, Reward = 54.0\n",
      "Episode 35, Reward = 12.0\n",
      "Episode 36, Reward = 16.0\n",
      "Episode 37, Reward = 24.0\n",
      "Episode 38, Reward = 31.0\n",
      "Episode 39, Reward = 16.0\n",
      "Episode 40, Reward = 29.0\n",
      "Episode 41, Reward = 23.0\n",
      "Episode 42, Reward = 13.0\n",
      "Episode 43, Reward = 36.0\n",
      "Episode 44, Reward = 60.0\n",
      "Episode 45, Reward = 25.0\n",
      "Episode 46, Reward = 34.0\n",
      "Episode 47, Reward = 23.0\n",
      "Episode 48, Reward = 70.0\n",
      "Episode 49, Reward = 26.0\n",
      "Episode 50, Reward = 99.0\n",
      "Episode 51, Reward = 13.0\n",
      "Episode 52, Reward = 13.0\n",
      "Episode 53, Reward = 25.0\n",
      "Episode 54, Reward = 27.0\n",
      "Episode 55, Reward = 27.0\n",
      "Episode 56, Reward = 51.0\n",
      "Episode 57, Reward = 25.0\n",
      "Episode 58, Reward = 57.0\n",
      "Episode 59, Reward = 49.0\n",
      "Episode 60, Reward = 12.0\n",
      "Episode 61, Reward = 49.0\n",
      "Episode 62, Reward = 17.0\n",
      "Episode 63, Reward = 17.0\n",
      "Episode 64, Reward = 63.0\n",
      "Episode 65, Reward = 75.0\n",
      "Episode 66, Reward = 84.0\n",
      "Episode 67, Reward = 45.0\n",
      "Episode 68, Reward = 14.0\n",
      "Episode 69, Reward = 112.0\n",
      "Episode 70, Reward = 58.0\n",
      "Episode 71, Reward = 18.0\n",
      "Episode 72, Reward = 33.0\n",
      "Episode 73, Reward = 64.0\n",
      "Episode 74, Reward = 63.0\n",
      "Episode 75, Reward = 43.0\n",
      "Episode 76, Reward = 45.0\n",
      "Episode 77, Reward = 108.0\n",
      "Episode 78, Reward = 57.0\n",
      "Episode 79, Reward = 97.0\n",
      "Episode 80, Reward = 148.0\n",
      "Episode 81, Reward = 125.0\n",
      "Episode 82, Reward = 127.0\n",
      "Episode 83, Reward = 122.0\n",
      "Episode 84, Reward = 110.0\n",
      "Episode 85, Reward = 140.0\n",
      "Episode 86, Reward = 41.0\n",
      "Episode 87, Reward = 167.0\n",
      "Episode 88, Reward = 51.0\n",
      "Episode 89, Reward = 58.0\n",
      "Episode 90, Reward = 155.0\n",
      "Episode 91, Reward = 273.0\n",
      "Episode 92, Reward = 121.0\n",
      "Episode 93, Reward = 148.0\n",
      "Episode 94, Reward = 64.0\n",
      "Episode 95, Reward = 112.0\n",
      "Episode 96, Reward = 15.0\n",
      "Episode 97, Reward = 23.0\n",
      "Episode 98, Reward = 121.0\n",
      "Episode 99, Reward = 59.0\n",
      "Episode 100, Reward = 22.0\n",
      "Episode 101, Reward = 68.0\n",
      "Episode 102, Reward = 74.0\n",
      "Episode 103, Reward = 46.0\n",
      "Episode 104, Reward = 89.0\n",
      "Episode 105, Reward = 21.0\n",
      "Episode 106, Reward = 15.0\n",
      "Episode 107, Reward = 23.0\n",
      "Episode 108, Reward = 43.0\n",
      "Episode 109, Reward = 34.0\n",
      "Episode 110, Reward = 80.0\n",
      "Episode 111, Reward = 42.0\n",
      "Episode 112, Reward = 17.0\n",
      "Episode 113, Reward = 107.0\n",
      "Episode 114, Reward = 121.0\n",
      "Episode 115, Reward = 25.0\n",
      "Episode 116, Reward = 83.0\n",
      "Episode 117, Reward = 12.0\n",
      "Episode 118, Reward = 125.0\n",
      "Episode 119, Reward = 36.0\n",
      "Episode 120, Reward = 84.0\n",
      "Episode 121, Reward = 195.0\n",
      "Episode 122, Reward = 138.0\n",
      "Episode 123, Reward = 190.0\n",
      "Episode 124, Reward = 51.0\n",
      "Episode 125, Reward = 258.0\n",
      "Episode 126, Reward = 270.0\n",
      "Episode 127, Reward = 259.0\n",
      "Episode 128, Reward = 212.0\n",
      "Episode 129, Reward = 390.0\n",
      "Episode 130, Reward = 258.0\n",
      "Episode 131, Reward = 215.0\n",
      "Episode 132, Reward = 217.0\n",
      "Episode 133, Reward = 156.0\n",
      "Episode 134, Reward = 137.0\n",
      "Episode 135, Reward = 123.0\n",
      "Episode 136, Reward = 215.0\n",
      "Episode 137, Reward = 178.0\n",
      "Episode 138, Reward = 128.0\n",
      "Episode 139, Reward = 141.0\n",
      "Episode 140, Reward = 106.0\n",
      "Episode 141, Reward = 199.0\n",
      "Episode 142, Reward = 203.0\n",
      "Episode 143, Reward = 170.0\n",
      "Episode 144, Reward = 202.0\n",
      "Episode 145, Reward = 186.0\n",
      "Episode 146, Reward = 146.0\n",
      "Episode 147, Reward = 156.0\n",
      "Episode 148, Reward = 164.0\n",
      "Episode 149, Reward = 123.0\n",
      "Episode 150, Reward = 125.0\n",
      "Episode 151, Reward = 115.0\n",
      "Episode 152, Reward = 127.0\n",
      "Episode 153, Reward = 21.0\n",
      "Episode 154, Reward = 44.0\n",
      "Episode 155, Reward = 24.0\n",
      "Episode 156, Reward = 115.0\n",
      "Episode 157, Reward = 100.0\n",
      "Episode 158, Reward = 111.0\n",
      "Episode 159, Reward = 116.0\n",
      "Episode 160, Reward = 24.0\n",
      "Episode 161, Reward = 105.0\n",
      "Episode 162, Reward = 132.0\n",
      "Episode 163, Reward = 16.0\n",
      "Episode 164, Reward = 123.0\n",
      "Episode 165, Reward = 113.0\n",
      "Episode 166, Reward = 143.0\n",
      "Episode 167, Reward = 126.0\n",
      "Episode 168, Reward = 147.0\n",
      "Episode 169, Reward = 114.0\n",
      "Episode 170, Reward = 130.0\n",
      "Episode 171, Reward = 142.0\n",
      "Episode 172, Reward = 148.0\n",
      "Episode 173, Reward = 133.0\n",
      "Episode 174, Reward = 161.0\n",
      "Episode 175, Reward = 213.0\n",
      "Episode 176, Reward = 207.0\n",
      "Episode 177, Reward = 223.0\n",
      "Episode 178, Reward = 231.0\n",
      "Episode 179, Reward = 246.0\n",
      "Episode 180, Reward = 321.0\n",
      "Episode 181, Reward = 343.0\n",
      "Episode 182, Reward = 460.0\n",
      "Episode 183, Reward = 352.0\n",
      "Episode 184, Reward = 500.0\n",
      "Solved CartPole!\n"
     ]
    }
   ],
   "source": [
    "# 開始訓練\n",
    "trained_policy = train_cartpole(max_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test over 10 episodes, average reward: 451.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "451.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(trained_policy, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 132.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rlenv/lib/python3.12/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "def play_cartpole(policy_net):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = policy_net.get_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "play_cartpole(trained_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
